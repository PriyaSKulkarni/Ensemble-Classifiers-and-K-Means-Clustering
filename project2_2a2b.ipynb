{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJTy8YSE2swh",
        "outputId": "f2195f43-d729-4e50-9e11-87e217a0d83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                 height            diameter             weight  \\\n",
            "0      0.12493777936236    0.13190375787948   0.38616824929558   \n",
            "1      0.14492137354649    0.12500746617994               0.75   \n",
            "2     0.071892497830642                0.03    0.1631761874001   \n",
            "3     0.084260612354435   0.037681879898677   0.23151165834924   \n",
            "4                  0.07   0.090347950123449   0.19177699658224   \n",
            "..                  ...                 ...                ...   \n",
            "115    0.15324808700145     0.1301638325634   0.40118457408591   \n",
            "116    0.12021522833926    0.11607358210855   0.33882777489585   \n",
            "117    0.07556917126148   0.072067954099429   0.16160560001853   \n",
            "118    0.13137129130651   0.091788907990009    0.2781412506351   \n",
            "119    0.12825404991764    0.14820139358115   0.45533536592389   \n",
            "\n",
            "                   hue      class  \n",
            "0      2.9517669794008   Plastic   \n",
            "1      3.4378755151121   Ceramic   \n",
            "2      4.0521528606463   Plastic   \n",
            "3      6.2831853071796   Ceramic   \n",
            "4      2.1255038511543   Plastic   \n",
            "..                 ...        ...  \n",
            "115    2.1792391626888   Plastic   \n",
            "116   0.87686630388415   Plastic   \n",
            "117    3.3382783895954   Plastic   \n",
            "118    2.3833925805427   Plastic   \n",
            "119    2.6866075609372   Plastic   \n",
            "\n",
            "[120 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# I have used my google drive to load the training and testing data\n",
        "# Upload the dataset file in your google drive and change the path to run the below line\n",
        "data= pd.read_csv('/content/drive/MyDrive/ML/Priyadataset2c.csv',dtype='object')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using first 100 records as training set and remaining 20 records as test data set"
      ],
      "metadata": {
        "id": "pM37iliaeZom"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ixtK3Ylm25o7",
        "outputId": "52359631-8aa7-48b2-a8e3-35c3017efa0b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                height            diameter             weight  \\\n",
            "0     0.12493777936236    0.13190375787948   0.38616824929558   \n",
            "1     0.14492137354649    0.12500746617994               0.75   \n",
            "2    0.071892497830642                0.03    0.1631761874001   \n",
            "3    0.084260612354435   0.037681879898677   0.23151165834924   \n",
            "4                 0.07   0.090347950123449   0.19177699658224   \n",
            "..                 ...                 ...                ...   \n",
            "95                0.05   0.063594729215563   0.13143726276907   \n",
            "96    0.14203794136301    0.11033482248778               0.75   \n",
            "97   0.059841975005583     0.1196374053952   0.43597686894734   \n",
            "98    0.11622427723581   0.089264682323942   0.64108492044275   \n",
            "99    0.10430398237586   0.097649309187052   0.40579556041064   \n",
            "\n",
            "                 hue  \n",
            "0    2.9517669794008  \n",
            "1    3.4378755151121  \n",
            "2    4.0521528606463  \n",
            "3    6.2831853071796  \n",
            "4    2.1255038511543  \n",
            "..               ...  \n",
            "95   3.6012437646385  \n",
            "96   2.9244966386833  \n",
            "97   2.3901625485212  \n",
            "98   4.3971330552283  \n",
            "99   3.4899266643618  \n",
            "\n",
            "[100 rows x 4 columns]\n",
            "                 height            diameter             weight  \\\n",
            "100   0.085872569171605   0.060071707631271   0.23795234540595   \n",
            "101    0.14040089151344    0.12124752013903   0.37599121579319   \n",
            "102    0.10083908716999   0.055441234834316   0.20709854846817   \n",
            "103   0.072207411902859   0.084926451828747   0.18521058741079   \n",
            "104    0.11353045366996   0.095215618141647    0.6470746972318   \n",
            "105    0.10566088151504    0.11856672459344               0.75   \n",
            "106     0.1069974602134   0.064677016327898   0.45854495136489   \n",
            "107                0.07    0.05333500068328   0.10290681906842   \n",
            "108   0.081078258906168   0.093040424908941   0.47344876241086   \n",
            "109    0.10793206231286     0.1034202057122   0.73437965619839   \n",
            "110    0.12385462975748    0.14144957071825               0.75   \n",
            "111   0.067885282469908   0.063274116572313   0.30492324914881   \n",
            "112    0.10801263294505    0.11322698725075               0.75   \n",
            "113    0.10547899463241   0.068535074817086   0.18052128046589   \n",
            "114    0.10524247704442   0.081726258784635   0.22822962111599   \n",
            "115    0.15324808700145     0.1301638325634   0.40118457408591   \n",
            "116    0.12021522833926    0.11607358210855   0.33882777489585   \n",
            "117    0.07556917126148   0.072067954099429   0.16160560001853   \n",
            "118    0.13137129130651   0.091788907990009    0.2781412506351   \n",
            "119    0.12825404991764    0.14820139358115   0.45533536592389   \n",
            "\n",
            "                   hue  \n",
            "100    4.4238050010281  \n",
            "101    5.5101988921737  \n",
            "102    1.6339199534662  \n",
            "103    3.2804509732644  \n",
            "104    3.6126248255231  \n",
            "105    2.2217807164398  \n",
            "106    3.1399261099439  \n",
            "107    4.5387065674884  \n",
            "108    2.7535486480091  \n",
            "109     4.598103043599  \n",
            "110    4.1724686054461  \n",
            "111    4.1804787289278  \n",
            "112    2.2193355020847  \n",
            "113    3.9968983570854  \n",
            "114    3.6558028671036  \n",
            "115    2.1792391626888  \n",
            "116   0.87686630388415  \n",
            "117    3.3382783895954  \n",
            "118    2.3833925805427  \n",
            "119    2.6866075609372  \n"
          ]
        }
      ],
      "source": [
        "X_train=data.iloc[0:100,:-1]\n",
        "print(X_train)\n",
        "X_test = data.iloc[100:,:-1]\n",
        "print(X_test)\n",
        "X_test=X_test.astype('float64')\n",
        "X_train=X_train.astype('float64')\n",
        "X = data.iloc[:,:-1]\n",
        "Y = data.iloc[:,-1]\n",
        "x_train = X.iloc[0:100,:].values\n",
        "x_test = X.iloc[100:,:].values\n",
        "y_train = Y[0:100].values\n",
        "y_test = Y[100:].values\n",
        "x_test=x_test.astype('float64')\n",
        "x_train=x_train.astype('float64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "B_ncDIwb3DIz"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_train)):\n",
        "    if y_train[i] == ' Plastic ':\n",
        "        y_train[i] = 0\n",
        "    elif y_train[i] == ' Metal ':\n",
        "        y_train[i] = 1\n",
        "    elif y_train[i] == ' Ceramic ':\n",
        "        y_train[i] = 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7eeQqWRC5Rka",
        "outputId": "dabd2bf8-97e0-49cb-bf2d-abc93c06deda"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 2, 0, 2, 0, 2, 2, 1, 0, 1, 2, 0, 0, 0, 1, 1, 2, 2, 0, 2, 1, 2,\n",
              "       2, 2, 1, 0, 1, 0, 1, 1, 1, 2, 1, 1, 0, 0, 0, 1, 2, 2, 1, 1, 0, 0,\n",
              "       1, 2, 2, 0, 0, 2, 1, 1, 2, 2, 0, 2, 1, 1, 1, 0, 2, 2, 1, 2, 0, 2,\n",
              "       0, 2, 2, 0, 1, 1, 2, 0, 0, 2, 0, 0, 2, 2, 0, 1, 0, 0, 2, 2, 0, 0,\n",
              "       0, 1, 1, 2, 2, 0, 0, 1, 2, 2, 2, 1], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "fKzjA99k3Zd4"
      },
      "outputs": [],
      "source": [
        "for i in range(len(y_test)):\n",
        "    if y_test[i] == ' Plastic ':\n",
        "        y_test[i] = 0\n",
        "    elif y_test[i] == ' Metal ':\n",
        "        y_test[i] = 1\n",
        "    elif y_test[i] == ' Ceramic ':\n",
        "        y_test[i] = 2 "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HyT6nb865gLC",
        "outputId": "45ef0112-73f1-41a5-ec9a-6142d59faa8b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 0, 0, 2, 2, 2, 0, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 0],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ],
      "source": [
        "y_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Regression classifier from project 1 which I implemented:"
      ],
      "metadata": {
        "id": "LZ34KD0IcscQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "yzfS0Yg33bXq"
      },
      "outputs": [],
      "source": [
        "# doing one hot coding\n",
        "# representing the y values in the form of a matrix and c represent the number classes\n",
        "# the matrix is of size  len(y) and no. of classes\n",
        "def one_hot(y, c):\n",
        "    y_hot=(np.arange(np.max(y) + 1) == y[:, None]).astype(float)\n",
        "    return y_hot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "-HvNffBR3gPt"
      },
      "outputs": [],
      "source": [
        "# implementing the softmax regression probability function\n",
        "def softmax(a):\n",
        "    Pro_exp = np.exp(a - np.max(a))\n",
        "    for i in range(len(a)):\n",
        "        Pro_exp[i] /= np.sum(Pro_exp[i])\n",
        "    return Pro_exp"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "zSyWFdmN3hnF"
      },
      "outputs": [],
      "source": [
        "# using xtraining data to train the algorithm\n",
        "# fitting the function by using random values for weight and passing 1 as bias and updating the graident\n",
        "def fit(X, y, lr, c, epochs):\n",
        "    m, n = X.shape\n",
        "    # Initializing weights randomly.\n",
        "    w = np.random.uniform(low=-0.1, high=0.1, size=(n,c))\n",
        "    b = np.array([1, 1, 1])\n",
        "    for epoch in range(epochs):\n",
        "        y_hot = one_hot(y, c)\n",
        "        z_val = np.dot(X,w)+ b\n",
        "        y_pred = softmax(z_val)\n",
        "        w_gradient = (1/m)*np.dot(X.T, (y_pred - y_hot)) \n",
        "        w = w - lr*w_gradient\n",
        "    return w"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "1x69hO3n3joy"
      },
      "outputs": [],
      "source": [
        "# prediction function\n",
        "def predict(X, w):\n",
        "    b = np.array([1, 1, 1])\n",
        "    z_val = X@w + b\n",
        "    y_predictions = softmax(z_val)\n",
        "    return np.argmax(y_predictions, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "CZouzbR43lLc"
      },
      "outputs": [],
      "source": [
        "# calculate the accuracy\n",
        "def Accuracy(y, y_prds):\n",
        "    return (np.sum(y==y_prds)/len(y))*100"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without using Boosting - (Single Classifier)"
      ],
      "metadata": {
        "id": "g5a7JNuUlFsn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clsr = fit(x_train, y_train, lr=0.8, c=3, epochs=6000)\n",
        "#print(clsr)"
      ],
      "metadata": {
        "id": "TGjapeKNk-Br"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred=(predict(x_test, clsr))\n",
        "print(test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6WZWc_1Vk-Go",
        "outputId": "d574a2eb-e822-4893-c59b-723ead6dd23d"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 2 2 2 0 2 2 2 2 2 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy : {Accuracy(y_test, test_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b59BRZijk-MA",
        "outputId": "022c8436-fdfc-4283-8b6b-b908a248e066"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 95.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Boosting on softmax regression"
      ],
      "metadata": {
        "id": "78R6pfCblLOG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2a) Implementing AdaBoost on top of softmax regression classifier  (softmax regression classifier that i used in project 1)\n",
        "\n",
        "2b) Applying boosting 10, 25, and 50 times to the training data. "
      ],
      "metadata": {
        "id": "oy6LhCltcQ7l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b839waT53nw3",
        "outputId": "8623a926-e583-484a-9b29-9b8a613dff41"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Boosting value: 10 for softmax regression has Accuracy of: 95.0\n",
            "Boosting value: 25 for softmax regression has Accuracy of: 35.0\n",
            "Boosting value: 50 for softmax regression has Accuracy of: 10.0\n"
          ]
        }
      ],
      "source": [
        "from operator import index\n",
        "# Boosting values\n",
        "boost_values = [10, 25, 50]\n",
        "train_data =data.iloc[0:100,:].astype('float64')\n",
        "for i in boost_values:\n",
        "    w_data = train_data.copy()\n",
        "    #print(w_data.shape)\n",
        "    # initial weights \n",
        "    N = w_data.iloc[:,0:4].shape[0]\n",
        "    X =w_data.iloc[:,0:4].values\n",
        "    lt_clrs = []\n",
        "    lt_alpha= []\n",
        "    for l in range(i):\n",
        "      #print(w_data.iloc[:,4:5].values)\n",
        "      weights = 1/len(train_data)\n",
        "      w_data['weights'] = weights \n",
        "      # fitting the softmax regression\n",
        "      clr = fit(X,y_train, lr=0.8, c=3, epochs=6000)\n",
        "      #print(clr)\n",
        "      train_ = w_data.iloc[:,0:4].values \n",
        "      train_l = w_data.iloc[:,4:5].values \n",
        "      train_= train_.astype('float64')\n",
        "      y_pred= predict(train_, clr)\n",
        "      err =[]\n",
        "      # calculating the error value\n",
        "      for e in range(len(y_train)):\n",
        "        if(y_pred[e] == y_train[e]):\n",
        "          err.append(0)\n",
        "        else:\n",
        "          err.append(1)  \n",
        "      w = w_data['weights'].values\n",
        "      # epsilon value\n",
        "      ep = sum(w * err)/w.sum()\n",
        "      #print(ep)\n",
        "      if 0 < ep < (1/2):\n",
        "        lt_clrs.append(clr) # appending the classifier\n",
        "        a = 0.5 * np.log((1-ep)/ep) # alpha value\n",
        "        lt_alpha.append(a)\n",
        "        weights = weights * np.exp(-a * np.asarray(train_l) * np.asarray(y_pred)) # adjusting weights based on alpha value\n",
        "        w_data['weights'] = weights\n",
        "      \n",
        "      w1 = w_data['weights'].values/ np.sum(w_data['weights'].values)\n",
        "      indices = np.random.choice(np.arange(N), size=N, p=w1)\n",
        "\n",
        "      X = X[indices]\n",
        "      y_train = y_train[indices]\n",
        "      \n",
        "    u=0\n",
        "    t=[]\n",
        "    c=0\n",
        "    for cr in lt_clrs:\n",
        "      pred=predict(x_test,cr)\n",
        "      # multiplying the alpha value with the predicted value of test data\n",
        "      t.append(np.asarray(pred)* lt_alpha[u])\n",
        "      u +=1\n",
        "    for ex in range(len(y_test)):\n",
        "      if(pred[ex] == y_test[ex]):\n",
        "          c +=1\n",
        "    #print(c)     \n",
        "    accuracy =(c/len(y_test))*100\n",
        "    print(f'Boosting value: {i} for softmax regression has Accuracy of: {accuracy}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before implementing Boosting,(single classifier) - the accuracy was 95%. \n",
        "After implementing Boosting, for the boosting value 10 , the accuracy was 95 % which showed good performance, but for the boosting value 25 and 50, the accuracy was reduced , the accuracy has been reduced as seen in the above result.the accuracy depends on the dataset it chooses based on the weight. Hence, Implementation of Boosting depends on the boosting value."
      ],
      "metadata": {
        "id": "X0cFX5EklSNS"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}