{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "6XI7UasbjspB",
        "outputId": "866fc16d-8d03-4536-e9d2-0a16a4e108a9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "                 height            diameter             weight  \\\n",
            "0      0.12493777936236    0.13190375787948   0.38616824929558   \n",
            "1      0.14492137354649    0.12500746617994               0.75   \n",
            "2     0.071892497830642                0.03    0.1631761874001   \n",
            "3     0.084260612354435   0.037681879898677   0.23151165834924   \n",
            "4                  0.07   0.090347950123449   0.19177699658224   \n",
            "..                  ...                 ...                ...   \n",
            "115    0.15324808700145     0.1301638325634   0.40118457408591   \n",
            "116    0.12021522833926    0.11607358210855   0.33882777489585   \n",
            "117    0.07556917126148   0.072067954099429   0.16160560001853   \n",
            "118    0.13137129130651   0.091788907990009    0.2781412506351   \n",
            "119    0.12825404991764    0.14820139358115   0.45533536592389   \n",
            "\n",
            "                   hue      class  \n",
            "0      2.9517669794008   Plastic   \n",
            "1      3.4378755151121   Ceramic   \n",
            "2      4.0521528606463   Plastic   \n",
            "3      6.2831853071796   Ceramic   \n",
            "4      2.1255038511543   Plastic   \n",
            "..                 ...        ...  \n",
            "115    2.1792391626888   Plastic   \n",
            "116   0.87686630388415   Plastic   \n",
            "117    3.3382783895954   Plastic   \n",
            "118    2.3833925805427   Plastic   \n",
            "119    2.6866075609372   Plastic   \n",
            "\n",
            "[120 rows x 5 columns]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "# I have used my google drive to load the training and testing data\n",
        "# Upload the dataset file in your google drive and change the path to run the below line\n",
        "data= pd.read_csv('/content/drive/MyDrive/ML/Priyadataset2c.csv',dtype='object')\n",
        "print(data)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using first 100 records as training set and remaining 20 records as test data set"
      ],
      "metadata": {
        "id": "36fQFOMydK-o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=data.iloc[0:100,:-1]\n",
        "print(X_train)\n",
        "X_test = data.iloc[100:,:-1]\n",
        "print(X_test)\n",
        "X_test=X_test.astype('float64')\n",
        "X_train=X_train.astype('float64')\n",
        "X = data.iloc[:,:-1]\n",
        "Y = data.iloc[:,-1]\n",
        "x_train = X.iloc[0:100,:].values\n",
        "x_test = X.iloc[100:,:].values\n",
        "y_train = Y[0:100].values\n",
        "y_test = Y[100:].values\n",
        "x_test=x_test.astype('float64')\n",
        "x_train=x_train.astype('float64')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "qp6ESOJHkJX4",
        "outputId": "11f5a963-159d-4908-986d-c9350ff7be24"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                height            diameter             weight  \\\n",
            "0     0.12493777936236    0.13190375787948   0.38616824929558   \n",
            "1     0.14492137354649    0.12500746617994               0.75   \n",
            "2    0.071892497830642                0.03    0.1631761874001   \n",
            "3    0.084260612354435   0.037681879898677   0.23151165834924   \n",
            "4                 0.07   0.090347950123449   0.19177699658224   \n",
            "..                 ...                 ...                ...   \n",
            "95                0.05   0.063594729215563   0.13143726276907   \n",
            "96    0.14203794136301    0.11033482248778               0.75   \n",
            "97   0.059841975005583     0.1196374053952   0.43597686894734   \n",
            "98    0.11622427723581   0.089264682323942   0.64108492044275   \n",
            "99    0.10430398237586   0.097649309187052   0.40579556041064   \n",
            "\n",
            "                 hue  \n",
            "0    2.9517669794008  \n",
            "1    3.4378755151121  \n",
            "2    4.0521528606463  \n",
            "3    6.2831853071796  \n",
            "4    2.1255038511543  \n",
            "..               ...  \n",
            "95   3.6012437646385  \n",
            "96   2.9244966386833  \n",
            "97   2.3901625485212  \n",
            "98   4.3971330552283  \n",
            "99   3.4899266643618  \n",
            "\n",
            "[100 rows x 4 columns]\n",
            "                 height            diameter             weight  \\\n",
            "100   0.085872569171605   0.060071707631271   0.23795234540595   \n",
            "101    0.14040089151344    0.12124752013903   0.37599121579319   \n",
            "102    0.10083908716999   0.055441234834316   0.20709854846817   \n",
            "103   0.072207411902859   0.084926451828747   0.18521058741079   \n",
            "104    0.11353045366996   0.095215618141647    0.6470746972318   \n",
            "105    0.10566088151504    0.11856672459344               0.75   \n",
            "106     0.1069974602134   0.064677016327898   0.45854495136489   \n",
            "107                0.07    0.05333500068328   0.10290681906842   \n",
            "108   0.081078258906168   0.093040424908941   0.47344876241086   \n",
            "109    0.10793206231286     0.1034202057122   0.73437965619839   \n",
            "110    0.12385462975748    0.14144957071825               0.75   \n",
            "111   0.067885282469908   0.063274116572313   0.30492324914881   \n",
            "112    0.10801263294505    0.11322698725075               0.75   \n",
            "113    0.10547899463241   0.068535074817086   0.18052128046589   \n",
            "114    0.10524247704442   0.081726258784635   0.22822962111599   \n",
            "115    0.15324808700145     0.1301638325634   0.40118457408591   \n",
            "116    0.12021522833926    0.11607358210855   0.33882777489585   \n",
            "117    0.07556917126148   0.072067954099429   0.16160560001853   \n",
            "118    0.13137129130651   0.091788907990009    0.2781412506351   \n",
            "119    0.12825404991764    0.14820139358115   0.45533536592389   \n",
            "\n",
            "                   hue  \n",
            "100    4.4238050010281  \n",
            "101    5.5101988921737  \n",
            "102    1.6339199534662  \n",
            "103    3.2804509732644  \n",
            "104    3.6126248255231  \n",
            "105    2.2217807164398  \n",
            "106    3.1399261099439  \n",
            "107    4.5387065674884  \n",
            "108    2.7535486480091  \n",
            "109     4.598103043599  \n",
            "110    4.1724686054461  \n",
            "111    4.1804787289278  \n",
            "112    2.2193355020847  \n",
            "113    3.9968983570854  \n",
            "114    3.6558028671036  \n",
            "115    2.1792391626888  \n",
            "116   0.87686630388415  \n",
            "117    3.3382783895954  \n",
            "118    2.3833925805427  \n",
            "119    2.6866075609372  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_train)):\n",
        "    if y_train[i] == ' Plastic ':\n",
        "        y_train[i] = 0\n",
        "    elif y_train[i] == ' Metal ':\n",
        "        y_train[i] = 1\n",
        "    elif y_train[i] == ' Ceramic ':\n",
        "        y_train[i] = 2"
      ],
      "metadata": {
        "id": "arpUfae0nULn"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(len(y_test)):\n",
        "    if y_test[i] == ' Plastic ':\n",
        "        y_test[i] = 0\n",
        "    elif y_test[i] == ' Metal ':\n",
        "        y_test[i] = 1\n",
        "    elif y_test[i] == ' Ceramic ':\n",
        "        y_test[i] = 2"
      ],
      "metadata": {
        "id": "LjRbpHJjxSDj"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Softmax Regression classifier from project 1 which I implemented"
      ],
      "metadata": {
        "id": "8LmUtVmVdU2H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# doing one hot coding\n",
        "# representing the y values in the form of a matrix and c represent the number classes\n",
        "# the matrix is of size  len(y) and no. of classes\n",
        "def one_hot(y, c):\n",
        "    y_hot=(np.arange(np.max(y) + 1) == y[:, None]).astype(float)\n",
        "    return y_hot"
      ],
      "metadata": {
        "id": "dl2I9CGqnZRC"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# implementing the softmax regression probability function\n",
        "def softmax(a):\n",
        "    Pro_exp = np.exp(a - np.max(a))\n",
        "    for i in range(len(a)):\n",
        "        Pro_exp[i] /= np.sum(Pro_exp[i])\n",
        "    return Pro_exp"
      ],
      "metadata": {
        "id": "dS7YeEoZnc7F"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# using xtraining data to train the algorithm\n",
        "# fitting the function by using random values for weight and passing 1 as bias and updating the graident\n",
        "def fit(X, y, lr, c, epochs):\n",
        "    m, n = X.shape\n",
        "    # Initializing weights randomly.\n",
        "    w = np.random.uniform(low=-0.1, high=0.1, size=(n,c))\n",
        "    b = np.array([1, 1, 1])\n",
        "    for epoch in range(epochs):\n",
        "        y_hot = one_hot(y, c)\n",
        "        z_val = np.dot(X,w)+ b\n",
        "        y_pred = softmax(z_val)\n",
        "        w_gradient = (1/m)*np.dot(X.T, (y_pred - y_hot)) \n",
        "        w = w - lr*w_gradient\n",
        "    return w"
      ],
      "metadata": {
        "id": "kC6M3vWgnfGz"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prediction function\n",
        "def predict(X, w):\n",
        "    b = np.array([1, 1, 1])\n",
        "    z_val = X@w + b\n",
        "    y_predictions = softmax(z_val)\n",
        "    return np.argmax(y_predictions, axis=1)"
      ],
      "metadata": {
        "id": "MihqKUTEnfI_"
      },
      "execution_count": 64,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# calculate the accuracy\n",
        "def Accuracy(y, y_prds):\n",
        "    return (np.sum(y==y_prds)/len(y))*100"
      ],
      "metadata": {
        "id": "j9Cvknuc1a3M"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Without using Bagging -( single classifier)"
      ],
      "metadata": {
        "id": "sgqRh-EuQX9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clsr = fit(x_train, y_train, lr=0.8, c=3, epochs=6000)\n",
        "#print(clsr)"
      ],
      "metadata": {
        "id": "CM4yEMYxPyBf"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_pred=(predict(x_test, clsr))\n",
        "print(test_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "fbfZUnKfPywo",
        "outputId": "3ed7ca12-d263-4066-835f-2ffe4c48faa5"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0 0 0 0 2 2 2 0 2 2 2 2 2 0 0 0 0 0 0 0]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Accuracy : {Accuracy(y_test, test_pred)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cTKK3pdnPy3p",
        "outputId": "9b792a02-c16e-4521-98e7-6d7f79ab932f"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy : 95.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1a) Implementing a bagging routine for a softmax regression classifier. \n",
        "(softmax regression classifier that i used in project 1)"
      ],
      "metadata": {
        "id": "-N6jWDBcd7pb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Using Bagging on softmax regression"
      ],
      "metadata": {
        "id": "0HoqPBlTQc5c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# bagging function\n",
        "import collections\n",
        "def bagging(n_bg):\n",
        "    data = []\n",
        "    for i in range(n_bg):\n",
        "      # randomly selecting the datapoints from the training dataset\n",
        "        idx = np.random.randint(0,100,100)\n",
        "        dta= X_train.iloc[idx]\n",
        "        data.append(dta)\n",
        "    clr_list = []\n",
        "    for d in data:\n",
        "        x_train = d.iloc[:,0:4].values\n",
        "        clsr = fit(x_train, y_train, lr=0.8, c=3, epochs=6000)\n",
        "        # append the classifier\n",
        "        clr_list.append(clsr)\n",
        "    c = 0\n",
        "    predict1 =[]\n",
        "    for lc in clr_list:\n",
        "      e = predict(x_test, lc)\n",
        "      predict1.append(e)\n",
        "    #print(predict1)\n",
        "    result=[]\n",
        "    for j in range(len(y_test)):\n",
        "      pred_i=[p[j] for p in predict1]\n",
        "      f_pred= collections.Counter(pred_i).most_common(1)[0][0]\n",
        "      result.append(f_pred)\n",
        "    for k in range(len(y_test)):\n",
        "      if result[k] == y_test[k]:\n",
        "            c += 1\n",
        "    Acc = (c / len(y_test)) * 100\n",
        "    return Acc"
      ],
      "metadata": {
        "id": "XrB3VpNbjve0"
      },
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1b) Applying bagging 10, 50, and 100 times to the training data."
      ],
      "metadata": {
        "id": "ZkVqx0XdeKSg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n = [10, 50, 100]\n",
        "for n_bg in n:\n",
        "    Accuracy = bagging(n_bg)\n",
        "    print(f'Bagging value of : {n_bg} for softmax regression has Accuracy of : {Accuracy} ')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "nFbRyytSndsI",
        "outputId": "d16f84f8-b905-4b77-8763-b7ee4fea229d"
      },
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bagging value of : 10 for softmax regression has Accuracy of : 50.0 \n",
            "Bagging value of : 50 for softmax regression has Accuracy of : 55.00000000000001 \n",
            "Bagging value of : 100 for softmax regression has Accuracy of : 55.00000000000001 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before implementing bagging (i.e on single classifier), the accuracy was 95%.\n",
        "After implementing bagging, the accuracy is reduced as seen in above result.\n",
        "Implementation of bagging has decreased the accuracy rate for softmax regression. hence Bagging degraded the performance."
      ],
      "metadata": {
        "id": "X0989yQSQbVS"
      }
    }
  ]
}